{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Libraries"
      ],
      "metadata": {
        "id": "rRhQgxO-IAf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr5Lhz8EHfGv"
      },
      "outputs": [],
      "source": [
        "!pip install sparseml[ultralytics]\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "!pip install roboflow\n",
        "!pip install getpass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "id": "ss7ZORklH-4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Annotated Dataset from Roboflow"
      ],
      "metadata": {
        "id": "K5q4G2EfIelt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "API_KEY = getpass.getpass(\"Please enter the api key to the dataset on Roboflow: \")"
      ],
      "metadata": {
        "id": "NxHtnAUqIYWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "project = rf.workspace(\"tinycv\").project(\"pedestrian-detection-cctv\")\n",
        "dataset = project.version(1).download(\"yolov8\")"
      ],
      "metadata": {
        "id": "JOeJdw8KIpe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/Pedestrian-Detection-CCTV-1/data.yaml /content/"
      ],
      "metadata": {
        "id": "hioP9N-Ej-zO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sparseml.ultralytics.train --help"
      ],
      "metadata": {
        "id": "BuJUTSTpLYnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Transfer Learning\n",
        "### In this section, we are transfer learning from an already pruned Yolov8n model trained on the COCO128 dataset. This is similar to transfer learning from the base Yolov8n model available in Ultralytics.\n",
        "#### To apply the following sparse transfer learning recipe we use the sparseml.ultralytics.train CLI with the following arguments:\n",
        "- *model* : Used to set the initial pruned model taken in this example from the SparseZoo. It could be a different locally trained pruned base model.\n",
        "- *recipe* : Used to set a pruning and quantization recipe for the resulting model.\n",
        "- *data* : YAML file of the dataset\n",
        "- **args*: The rest of the arguments available in the Ultralytics.train API"
      ],
      "metadata": {
        "id": "FLt9rscvfLT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!sparseml.ultralytics.train \\\n",
        "--model \"zoo:cv/detection/yolov8-n/pytorch/ultralytics/coco/pruned49-none\" \\\n",
        "--recipe \"zoo:cv/detection/yolov8-n/pytorch/ultralytics/voc/pruned49_quant-none\" \\\n",
        "--data \"data.yaml\" \\\n",
        "--imgsz 224 \\\n",
        "--epochs 100"
      ],
      "metadata": {
        "id": "GpzI-YdYLx23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}